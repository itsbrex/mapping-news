name: Parse RSS Feeds

on:
  schedule:
    # Run every hour
    - cron: "*/30 13-22 * * *"
    - cron: "0 23,0-2,12 * * *"
    - cron: "0 3-11/2 * * *"
  workflow_dispatch: # Allow manual triggering

jobs:
  parse-feeds:
    runs-on: ubuntu-latest
    environment: production

    steps:
      # Step 1: Check out the repository
      - name: Checkout code
        uses: actions/checkout@v3

      # Step 2: Set up Python with Poetry
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: "3.12.8"
      - name: Install Poetry
        run: |
          pip install poetry

      # Step 3: Install dependencies
      - name: Install dependencies
        run: poetry install

      # Step 4: Download previous cache artifacts if they exist
      - name: Download cache artifacts
        uses: actions/download-artifact@v4
        with:
          name: cache-artifacts
          path: cache-artifacts
        continue-on-error: true

      # Step 5: Copy artifacts to cache directory if they exist
      - name: Prepare cache directory
        run: |
          mkdir -p cache
          if [ -d "cache-artifacts" ]; then
            cp -r cache-artifacts/* cache/ || true
          fi

      # Step 6: Run the feed parser script
      - name: Run feed parser script
        run: poetry run python actions/feedParser.py
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GOOGLE_MAPS_API_KEY: ${{ secrets.GOOGLE_MAPS_API_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_API_KEY: ${{ secrets.SUPABASE_API_KEY }}
          SUPABASE_SER_KEY: ${{ secrets.SUPABASE_SER_KEY }}

      # Step 7: Upload cache artifacts
      - name: Upload cache artifacts
        uses: actions/upload-artifact@v4
        with:
          name: cache-artifacts
          path: cache/
          retention-days: 7
